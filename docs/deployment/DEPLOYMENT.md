# üöÄ D√©ploiement de l'API Senchess sur Vercel

Ce guide vous explique comment d√©ployer votre API de d√©tection d'√©checs sur Vercel.

## üìã Pr√©requis

1. Un compte [Vercel](https://vercel.com) (gratuit)
2. Le [CLI Vercel](https://vercel.com/cli) install√© : `npm i -g vercel`
3. Votre mod√®le YOLO entra√Æn√© (fichier `.pt`)

## ‚ö†Ô∏è Important : Limitation des Mod√®les

**Probl√®me** : Les fichiers de mod√®les YOLO (`.pt`) sont trop volumineux pour √™tre d√©ploy√©s directement sur Vercel (limite de 250MB pour le d√©ploiement).

### Solutions possibles :

### Option 1 : H√©bergement externe du mod√®le (Recommand√©)

1. **H√©berger le mod√®le sur un service de stockage cloud** :
   - Google Cloud Storage
   - AWS S3
   - Azure Blob Storage
   - Hugging Face Hub

2. **T√©l√©charger le mod√®le au d√©marrage** :
   Modifiez `api/index.py` :

```python
import requests
from pathlib import Path

def load_model():
    """Charge le mod√®le YOLO depuis un URL"""
    global model
    
    model_url = os.environ.get('MODEL_URL')
    local_path = '/tmp/model.pt'
    
    # T√©l√©charger si pas d√©j√† pr√©sent
    if not os.path.exists(local_path):
        print(f"üì• T√©l√©chargement du mod√®le depuis {model_url}...")
        response = requests.get(model_url)
        with open(local_path, 'wb') as f:
            f.write(response.content)
        print("‚úÖ Mod√®le t√©l√©charg√©")
    
    model = YOLO(local_path)
```

### Option 2 : Utiliser Hugging Face

H√©bergez votre mod√®le sur Hugging Face :

```bash
# Installer huggingface_hub
pip install huggingface_hub

# Uploader le mod√®le
from huggingface_hub import HfApi
api = HfApi()
api.upload_file(
    path_or_fileobj="models/senchess_gear_v1.1/weights/best.pt",
    path_in_repo="best.pt",
    repo_id="votre-username/senchess-model",
    repo_type="model"
)
```

Puis dans `api/index.py` :

```python
from huggingface_hub import hf_hub_download

def load_model():
    global model
    model_path = hf_hub_download(
        repo_id="votre-username/senchess-model",
        filename="best.pt",
        cache_dir="/tmp"
    )
    model = YOLO(model_path)
```

### Option 3 : D√©ploiement sur une plateforme diff√©rente

Si vous avez besoin d'h√©berger des fichiers volumineux :
- **Railway** (support des volumes persistants)
- **Render** (d√©ploiement Docker)
- **Google Cloud Run**
- **AWS Lambda** (avec EFS)

## üõ†Ô∏è D√©ploiement sur Vercel

### √âtape 1 : Pr√©parer votre projet

1. Assurez-vous que tous les fichiers sont en place :
```
Senchess AI/
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ index.py          # API Flask
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt  # D√©pendances Python
‚îú‚îÄ‚îÄ vercel.json          # Configuration Vercel
‚îî‚îÄ‚îÄ .vercelignore        # Fichiers √† ignorer
```

2. H√©bergez votre mod√®le (voir options ci-dessus)

### √âtape 2 : Configuration des variables d'environnement

Cr√©ez un fichier `.env.local` (ne pas commiter) :

```bash
MODEL_URL=https://votre-url/model.pt
# ou
HF_REPO_ID=votre-username/senchess-model
```

### √âtape 3 : D√©ployer

```bash
# Depuis le r√©pertoire racine du projet
cd "/Users/macbookair/Desktop/Senchess AI"

# Login √† Vercel
vercel login

# D√©ployer en preview
vercel

# Ou d√©ployer en production directement
vercel --prod
```

### √âtape 4 : Configurer les variables d'environnement sur Vercel

Depuis le dashboard Vercel :
1. Allez dans votre projet ‚Üí Settings ‚Üí Environment Variables
2. Ajoutez :
   - `MODEL_URL` : URL de votre mod√®le
   - `MODEL_API_KEY` : (optionnel) pour s√©curiser l'acc√®s

## üß™ Tester l'API

### En local :

```bash
# Installer les d√©pendances
cd api
pip install -r requirements.txt

# Lancer l'API
python index.py

# L'API sera disponible sur http://localhost:5000
```

### Test avec curl :

```bash
# Health check
curl https://votre-app.vercel.app/health

# Pr√©diction avec une image
curl -X POST https://votre-app.vercel.app/predict \
  -F "image=@path/to/chess.jpg" \
  -F "conf=0.25"
```

### Test avec JavaScript (comme dans votre site) :

```typescript
// src/services/chessImageRecognition.ts

export async function analyzeChessBoardImage(
  imageUrl: string
): Promise<ChessPositionAnalysis> {
  try {
    // 1. Pr√©parer l'image
    const imageBlob = await fetch(imageUrl).then(r => r.blob());
    
    // 2. Cr√©er FormData
    const formData = new FormData();
    formData.append('image', imageBlob);
    formData.append('conf', '0.25');
    
    // 3. Appeler votre API Vercel
    const response = await fetch('https://votre-app.vercel.app/predict', {
      method: 'POST',
      body: formData,
      // headers: {
      //   'Authorization': `Bearer ${import.meta.env.VITE_MODEL_API_KEY}`
      // }
    });
    
    if (!response.ok) {
      throw new Error(`API Error: ${response.statusText}`);
    }
    
    const result = await response.json();
    
    // 4. Le r√©sultat contient d√©j√† le FEN !
    return {
      fen: result.fen,
      description: result.description,
      confidence: result.confidence > 0.9 ? 'high' : 
                  result.confidence > 0.7 ? 'medium' : 'low',
      detectedPieces: result.detectedPieces,
      warnings: result.warnings || []
    };
    
  } catch (error) {
    console.error('Model Recognition Error:', error);
    throw new Error('√âchec de reconnaissance du mod√®le');
  }
}
```

## üìä Format de r√©ponse de l'API

```json
{
  "success": true,
  "fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1",
  "pieces": [
    {
      "id": 1,
      "class": "white-king",
      "confidence": 0.95,
      "bbox": {
        "x1": 123.45,
        "y1": 234.56,
        "x2": 178.90,
        "y2": 289.12,
        "width": 55.45,
        "height": 54.56
      }
    }
  ],
  "confidence": 0.89,
  "detectedPieces": 32,
  "description": "Position d√©tect√©e avec 32 pi√®ces",
  "imageSize": {
    "width": 800,
    "height": 800
  },
  "warnings": []
}
```

## üîí S√©curit√© (Optionnel)

Pour prot√©ger votre API, ajoutez une authentification :

```python
# Dans api/index.py
from functools import wraps

def require_api_key(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        api_key = request.headers.get('Authorization', '').replace('Bearer ', '')
        expected_key = os.environ.get('API_KEY')
        
        if not expected_key or api_key != expected_key:
            return jsonify({'error': 'Unauthorized'}), 401
        
        return f(*args, **kwargs)
    return decorated_function

@app.route('/predict', methods=['POST'])
@require_api_key  # Ajouter ce d√©corateur
def predict():
    # ... reste du code
```

## üìà Monitoring

Sur Vercel, vous pouvez :
- Voir les logs en temps r√©el : `vercel logs`
- Monitorer les requ√™tes dans le dashboard
- Configurer des alertes

## üêõ D√©pannage

### Erreur : "Module not found"
- V√©rifiez que `api/requirements.txt` contient toutes les d√©pendances

### Erreur : "Model not loaded"
- V√©rifiez que `MODEL_URL` est correctement configur√©
- Testez l'URL du mod√®le dans un navigateur

### Timeout lors du t√©l√©chargement du mod√®le
- Vercel a une limite de 10s pour les fonctions serverless (gratuit)
- Passez au plan Pro pour 60s, ou utilisez une autre plateforme

### Erreur de m√©moire
- Les mod√®les YOLO sont gourmands en RAM
- Utilisez un mod√®le plus l√©ger (nano ou small)
- Consid√©rez une plateforme avec plus de RAM

## üöÄ Prochaines √©tapes

1. **Optimiser le mod√®le** : Utilisez ONNX ou TensorRT pour des inf√©rences plus rapides
2. **Caching** : Mettre en cache les pr√©dictions fr√©quentes
3. **Batch processing** : Supporter plusieurs images en une requ√™te
4. **WebSocket** : Pour des mises √† jour en temps r√©el
5. **CDN** : H√©berger les r√©sultats d'image annot√©es

## üìö Ressources

- [Vercel Python Runtime](https://vercel.com/docs/functions/runtimes/python)
- [Ultralytics YOLO Docs](https://docs.ultralytics.com/)
- [Flask Documentation](https://flask.palletsprojects.com/)
- [Hugging Face Hub](https://huggingface.co/docs/hub/index)

## üí° Alternative : API locale pour d√©veloppement

Si Vercel est trop complexe pour commencer, testez d'abord localement :

```bash
# Terminal 1 : Lancer l'API
cd api
python index.py

# Terminal 2 : Tester
curl -X POST http://localhost:5000/predict -F "image=@test.jpg"
```

Puis utilisez [ngrok](https://ngrok.com/) pour exposer temporairement :

```bash
ngrok http 5000
# Utilisez l'URL https fournie dans votre site
```
