"""
Script de fine-tuning pour crÃ©er Senchess Gear-Haki Ultimate
Combine les datasets et fine-tune depuis le meilleur modÃ¨le
"""
import argparse
from pathlib import Path
from ultralytics import YOLO
import yaml
import shutil
from datetime import datetime


def merge_datasets(gear_path, haki_path, output_path):
    """
    Fusionne les datasets Gear et Haki
    
    Args:
        gear_path: Chemin vers dataset Gear (processed)
        haki_path: Chemin vers dataset Haki (chess_decoder_1000)
        output_path: Chemin de sortie pour dataset fusionnÃ©
    """
    print("\n" + "="*70)
    print("ğŸ“¦ FUSION DES DATASETS")
    print("="*70)
    
    output_path = Path(output_path)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # CrÃ©er la structure
    for split in ['train', 'val', 'test']:
        (output_path / 'images' / split).mkdir(parents=True, exist_ok=True)
        (output_path / 'labels' / split).mkdir(parents=True, exist_ok=True)
    
    total_copied = 0
    
    # Copier Gear dataset
    print("\nğŸ¥ˆ Copie dataset Gear (photos physiques)...")
    gear_path = Path(gear_path)
    
    # Gear: train -> train, valid -> val, test -> test
    gear_mapping = {
        'train': 'train',
        'valid': 'val',
        'test': 'test'
    }
    
    for gear_split, output_split in gear_mapping.items():
        gear_images = gear_path / gear_split / 'images'
        gear_labels = gear_path / gear_split / 'labels'
        
        if gear_images.exists():
            images = list(gear_images.glob('*.jpg'))
            for img in images:
                # Copier image avec prÃ©fixe 'gear_'
                new_name = f"gear_{img.name}"
                shutil.copy(img, output_path / 'images' / output_split / new_name)
                
                # Copier label correspondant
                label = gear_labels / f"{img.stem}.txt"
                if label.exists():
                    shutil.copy(label, output_path / 'labels' / output_split / f"gear_{label.name}")
                
                total_copied += 1
            
            print(f"  {gear_split:6} -> {output_split:5} : {len(images)} images")
    
    # Copier Haki dataset
    print("\nğŸ¥‡ Copie dataset Haki (diagrammes 2D)...")
    haki_path = Path(haki_path)
    
    # Haki: train -> train, val -> val, test -> test
    for split in ['train', 'val', 'test']:
        haki_images = haki_path / 'images' / split
        haki_labels = haki_path / 'labels' / split
        
        if haki_images.exists():
            images = list(haki_images.glob('*.png')) + list(haki_images.glob('*.jpg'))
            for img in images:
                # Copier image avec prÃ©fixe 'haki_'
                new_name = f"haki_{img.name}"
                shutil.copy(img, output_path / 'images' / split / new_name)
                
                # Copier label correspondant
                label = haki_labels / f"{img.stem}.txt"
                if label.exists():
                    shutil.copy(label, output_path / 'labels' / split / f"haki_{label.name}")
                
                total_copied += 1
            
            print(f"  {split:6} -> {split:5} : {len(images)} images")
    
    # CrÃ©er data.yaml
    data_yaml = {
        'path': str(output_path.absolute()),
        'train': 'images/train',
        'val': 'images/val',
        'test': 'images/test',
        'nc': 13,
        'names': [
            'black-bishop', 'black-king', 'black-knight', 'black-pawn',
            'black-queen', 'black-rook', 'white-bishop', 'white-king',
            'white-knight', 'white-pawn', 'white-queen', 'white-rook',
            'chessboard'
        ]
    }
    
    with open(output_path / 'data.yaml', 'w') as f:
        yaml.dump(data_yaml, f, default_flow_style=False)
    
    print(f"\nâœ… Dataset fusionnÃ© crÃ©Ã© : {output_path}")
    print(f"   Total : {total_copied} fichiers copiÃ©s")
    
    # Statistiques
    train_count = len(list((output_path / 'images' / 'train').glob('*')))
    val_count = len(list((output_path / 'images' / 'val').glob('*')))
    test_count = len(list((output_path / 'images' / 'test').glob('*')))
    
    print(f"\nğŸ“Š RÃ©partition finale :")
    print(f"   Train : {train_count} images")
    print(f"   Val   : {val_count} images")
    print(f"   Test  : {test_count} images")
    print(f"   TOTAL : {train_count + val_count + test_count} images")
    
    return output_path / 'data.yaml'


def finetune_model(base_model, data_yaml, epochs, lr0, project_name):
    """
    Fine-tune un modÃ¨le existant
    
    Args:
        base_model: Chemin vers le modÃ¨le de base
        data_yaml: Chemin vers data.yaml
        epochs: Nombre d'epochs
        lr0: Learning rate initial
        project_name: Nom du projet/modÃ¨le
    """
    print("\n" + "="*70)
    print("ğŸš€ FINE-TUNING DU MODÃˆLE")
    print("="*70)
    
    print(f"\nModÃ¨le de base : {base_model}")
    print(f"Dataset        : {data_yaml}")
    print(f"Epochs         : {epochs}")
    print(f"Learning rate  : {lr0}")
    print(f"Nom du projet  : {project_name}")
    
    # Charger le modÃ¨le
    print("\nğŸ“¥ Chargement du modÃ¨le de base...")
    model = YOLO(base_model)
    
    # Configuration d'entraÃ®nement
    print("\nğŸ‹ï¸  DÃ©but du fine-tuning...\n")
    
    results = model.train(
        data=str(data_yaml),
        epochs=epochs,
        imgsz=640,
        batch=8,
        lr0=lr0,
        lrf=0.01,
        momentum=0.937,
        weight_decay=0.0005,
        warmup_epochs=3,
        warmup_momentum=0.8,
        warmup_bias_lr=0.1,
        box=7.5,
        cls=0.5,
        dfl=1.5,
        project='models',
        name=project_name,
        exist_ok=True,
        pretrained=True,
        optimizer='SGD',
        verbose=True,
        seed=0,
        deterministic=True,
        single_cls=False,
        rect=False,
        cos_lr=False,
        close_mosaic=10,
        resume=False,
        amp=True,
        fraction=1.0,
        profile=False,
        freeze=None,
        multi_scale=False,
        overlap_mask=True,
        mask_ratio=4,
        dropout=0.0,
        val=True,
        save=True,
        save_period=-1,
        cache=False,
        device='cpu',
        workers=4,
        plots=True,
        label_smoothing=0.0,
        patience=50,
        augment=True,
        hsv_h=0.015,
        hsv_s=0.7,
        hsv_v=0.4,
        degrees=0.0,
        translate=0.1,
        scale=0.5,
        shear=0.0,
        perspective=0.0,
        flipud=0.0,
        fliplr=0.5,
        mosaic=1.0,
        mixup=0.0,
        copy_paste=0.0
    )
    
    return results


def main():
    parser = argparse.ArgumentParser(description="Fine-tuning Senchess Gear-Haki")
    
    # Chemins datasets
    parser.add_argument('--gear-data', type=str, 
                       default='data/processed',
                       help='Chemin dataset Gear')
    parser.add_argument('--haki-data', type=str,
                       default='data/chess_decoder_1000',
                       help='Chemin dataset Haki')
    parser.add_argument('--output-data', type=str,
                       default='data/gear_haki_merged',
                       help='Chemin dataset fusionnÃ©')
    
    # ModÃ¨le de base
    parser.add_argument('--base-model', type=str,
                       default='models/senchess_haki_v1.0/weights/best.pt',
                       help='ModÃ¨le de base pour fine-tuning (haki par dÃ©faut)')
    
    # HyperparamÃ¨tres
    parser.add_argument('--epochs', type=int, default=50,
                       help='Nombre d\'epochs (dÃ©faut: 50)')
    parser.add_argument('--lr0', type=float, default=0.001,
                       help='Learning rate initial (dÃ©faut: 0.001)')
    
    # Nom du projet
    parser.add_argument('--name', type=str,
                       default='senchess_gear_haki_finetune',
                       help='Nom du modÃ¨le')
    
    # Options
    parser.add_argument('--skip-merge', action='store_true',
                       help='Skip dataset merge (utilise dataset existant)')
    
    args = parser.parse_args()
    
    print("\n" + "="*70)
    print("ğŸ¯ SENCHESS GEAR-HAKI FINE-TUNING")
    print("="*70)
    print(f"\nDate : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"StratÃ©gie : Fine-tune depuis {Path(args.base_model).stem}")
    print(f"Objectif  : ModÃ¨le universel 2D + 3D")
    
    # Ã‰tape 1 : Fusion des datasets (si nÃ©cessaire)
    if not args.skip_merge:
        data_yaml = merge_datasets(
            args.gear_data,
            args.haki_data,
            args.output_data
        )
    else:
        data_yaml = Path(args.output_data) / 'data.yaml'
        print(f"\nâ­ï¸  Fusion skippÃ©e, utilisation de : {data_yaml}")
    
    # Ã‰tape 2 : Fine-tuning
    results = finetune_model(
        args.base_model,
        data_yaml,
        args.epochs,
        args.lr0,
        args.name
    )
    
    # RÃ©sultats finaux
    print("\n" + "="*70)
    print("âœ… FINE-TUNING TERMINÃ‰")
    print("="*70)
    
    model_path = Path('models') / args.name / 'weights' / 'best.pt'
    print(f"\nğŸ“¦ ModÃ¨le sauvegardÃ© : {model_path}")
    print(f"ğŸ“Š RÃ©sultats : models/{args.name}/results.csv")
    print(f"ğŸ“ˆ Courbes : models/{args.name}/results.png")
    
    print("\nğŸ’¡ Pour Ã©valuer le modÃ¨le :")
    print(f"   python src/evaluate.py --model {model_path}")
    print(f"\nğŸ’¡ Pour prÃ©dire :")
    print(f"   python src/predict.py --model {model_path} --source imgTest/")
    
    print("\nğŸ‰ Fine-tuning Gear-Haki complÃ©tÃ© !")


if __name__ == '__main__':
    main()
