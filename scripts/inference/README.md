# Inference Scripts

Scripts pour utiliser les mod√®les entra√Æn√©s en production et tester leurs performances.

## Scripts Disponibles

### export_openvino.py
Export d'un mod√®le YOLO vers le format OpenVINO.

**Usage:**
```bash
python scripts/inference/export_openvino.py
```

**Sortie:** Dossier avec le mod√®le OpenVINO (ex: `best_openvino_model/`)

**Options du script:**
- Mod√®le source (best.pt)
- Format: OpenVINO IR
- Taille: 640x640
- FP16: D√©sactiv√© (meilleure compatibilit√©)

### benchmark_openvino_fixed.py ‚≠ê PRODUCTION
Benchmark complet avec l'API native OpenVINO (pas via Ultralytics).

**R√©sultats Intel Iris Xe:**
```
PyTorch CPU:    58.0ms / image ‚Üí 17.2 FPS (baseline)
OpenVINO CPU:   25.7ms / image ‚Üí 38.9 FPS (2.26x speedup)
OpenVINO GPU:    9.7ms / image ‚Üí 103.6 FPS (6.01x speedup) üöÄ
```

**Usage:**
```bash
python scripts/inference/benchmark_openvino_fixed.py
```

**Caract√©ristiques:**
- Preprocessing natif OpenVINO (pas PIL/PyTorch)
- Performance hint: LATENCY (optimal pour single image)
- 100 iterations de warmup
- 1000 iterations de benchmark
- Statistiques compl√®tes (mean, std, min, max)

### benchmark_openvino.py
Premier benchmark via l'API Ultralytics (plus lent).

**Note:** Utiliser `benchmark_openvino_fixed.py` √† la place.

### test_models.py
Tests de validation des mod√®les entra√Æn√©s.

**Usage:**
```bash
python scripts/inference/test_models.py
```

### analyze_image.py
Analyse d'images avec un mod√®le entra√Æn√©.

**Usage:**
```bash
python scripts/inference/analyze_image.py --image <path> --model <path>
```

## Workflow OpenVINO

### 1. Entra√Æner le mod√®le
```bash
python scripts/training/train_intel.py --quick
```

### 2. Exporter vers OpenVINO
```bash
python scripts/inference/export_openvino.py
```

### 3. Benchmarker les performances
```bash
python scripts/inference/benchmark_openvino_fixed.py
```

### 4. Utiliser en production
```python
import openvino as ov
import numpy as np

# Charger le mod√®le
core = ov.Core()
model = core.read_model("best_openvino_model/best.xml")
compiled = core.compile_model(model, "GPU", {"PERFORMANCE_HINT": "LATENCY"})

# Inference
input_data = np.random.randn(1, 3, 640, 640).astype(np.float32)
result = compiled([input_data])
```

## OpenVINO GPU Performance ‚úÖ

**Pourquoi OpenVINO GPU est 6x plus rapide?**

1. **Optimisations bas niveau**: OpenVINO compile le mod√®le sp√©cifiquement pour Intel Iris Xe
2. **Kernel fusion**: Combine plusieurs op√©rations en une seule
3. **Memory management**: Allocation m√©moire GPU optimis√©e
4. **Native preprocessing**: Pas de conversion PIL/PyTorch ‚Üí NumPy
5. **LATENCY hint**: Optimise pour inference single-image

**Comparaison:**
- PyTorch: CPU only, overhead Python √©lev√©
- OpenVINO CPU: Optimis√© mais reste sur CPU
- **OpenVINO GPU: Hardware acceleration + optimisations** üöÄ

## Cas d'Usage

### Development (Local)
- Test rapide: PyTorch CPU (simplicit√©)
- Validation: OpenVINO GPU (performances r√©elles)

### Production
- Webcam real-time: OpenVINO GPU (103 FPS)
- Batch processing: OpenVINO CPU (parall√©lisation)
- Cloud inference: PyTorch CUDA (si NVIDIA disponible)

### Embedded/Edge
- Intel NUC: OpenVINO GPU
- Raspberry Pi: OpenVINO CPU + NEON
- NVIDIA Jetson: PyTorch CUDA/TensorRT

## Documentation

Voir `OPENVINO_SUCCESS.md` √† la racine pour le guide complet.
