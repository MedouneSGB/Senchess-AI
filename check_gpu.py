"""
Script pour v√©rifier la disponibilit√© du GPU et les capacit√©s CUDA
"""

import torch
import sys

def check_gpu():
    """V√©rifie et affiche les informations sur le GPU"""
    
    print("\n" + "="*70)
    print("üîç V√âRIFICATION DU GPU ET CUDA")
    print("="*70 + "\n")
    
    # Version de PyTorch
    print(f"üì¶ Version de PyTorch : {torch.__version__}")
    
    # Disponibilit√© CUDA
    cuda_available = torch.cuda.is_available()
    print(f"\nüéÆ CUDA disponible : {'‚úÖ OUI' if cuda_available else '‚ùå NON'}")
    
    if cuda_available:
        # Version CUDA
        print(f"   Version CUDA : {torch.version.cuda}")
        print(f"   Version cuDNN : {torch.backends.cudnn.version()}")
        
        # Nombre de GPUs
        gpu_count = torch.cuda.device_count()
        print(f"\nüñ•Ô∏è  Nombre de GPUs : {gpu_count}")
        
        # Informations pour chaque GPU
        for i in range(gpu_count):
            print(f"\n   GPU {i}:")
            print(f"      Nom : {torch.cuda.get_device_name(i)}")
            
            # Capacit√© de m√©moire
            total_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
            print(f"      M√©moire totale : {total_memory:.2f} GB")
            
            # M√©moire disponible
            if torch.cuda.is_available():
                torch.cuda.set_device(i)
                allocated = torch.cuda.memory_allocated(i) / (1024**3)
                reserved = torch.cuda.memory_reserved(i) / (1024**3)
                free = total_memory - reserved
                print(f"      M√©moire allou√©e : {allocated:.2f} GB")
                print(f"      M√©moire r√©serv√©e : {reserved:.2f} GB")
                print(f"      M√©moire libre : {free:.2f} GB")
            
            # Capacit√© de calcul
            capability = torch.cuda.get_device_capability(i)
            print(f"      Compute Capability : {capability[0]}.{capability[1]}")
        
        # Test simple
        print("\nüß™ Test d'allocation GPU...")
        try:
            x = torch.rand(1000, 1000).cuda()
            y = torch.rand(1000, 1000).cuda()
            z = x @ y
            print("   ‚úÖ Test r√©ussi ! Le GPU fonctionne correctement.")
            
            # Benchmark simple
            import time
            print("\n‚ö° Benchmark simple (multiplication de matrices):")
            
            # CPU
            x_cpu = torch.rand(2000, 2000)
            y_cpu = torch.rand(2000, 2000)
            start = time.time()
            z_cpu = x_cpu @ y_cpu
            cpu_time = time.time() - start
            print(f"   CPU : {cpu_time:.4f} secondes")
            
            # GPU
            x_gpu = torch.rand(2000, 2000).cuda()
            y_gpu = torch.rand(2000, 2000).cuda()
            torch.cuda.synchronize()
            start = time.time()
            z_gpu = x_gpu @ y_gpu
            torch.cuda.synchronize()
            gpu_time = time.time() - start
            print(f"   GPU : {gpu_time:.4f} secondes")
            
            speedup = cpu_time / gpu_time
            print(f"   üöÄ Acc√©l√©ration : {speedup:.2f}x plus rapide")
            
        except Exception as e:
            print(f"   ‚ùå Erreur lors du test : {e}")
    
    else:
        print("\n‚ö†Ô∏è  Aucun GPU d√©tect√©. Raisons possibles :")
        print("   1. Aucun GPU NVIDIA install√©")
        print("   2. Drivers NVIDIA non install√©s ou obsol√®tes")
        print("   3. PyTorch install√© sans support CUDA")
        print("\nüí° Pour installer PyTorch avec CUDA :")
        print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
        print("   (Remplacez cu118 par votre version CUDA)")
    
    # Recommandations pour l'entra√Ænement
    print("\n" + "="*70)
    print("üìã RECOMMANDATIONS POUR L'ENTRA√éNEMENT")
    print("="*70 + "\n")
    
    if cuda_available:
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        if gpu_memory >= 8:
            print("‚úÖ Votre GPU a suffisamment de m√©moire pour l'entra√Ænement YOLOv8")
            print("   Batch size recommand√© : 16-32")
        elif gpu_memory >= 4:
            print("‚ö†Ô∏è  M√©moire GPU limit√©e")
            print("   Batch size recommand√© : 8-16")
        else:
            print("‚ö†Ô∏è  M√©moire GPU tr√®s limit√©e")
            print("   Batch size recommand√© : 4-8")
        
        print(f"\n   Avec GPU, l'entra√Ænement sera environ 10-20x plus rapide")
        print(f"   Temps estim√© : 30-90 minutes pour 50 epochs")
    else:
        print("‚ö†Ô∏è  Sans GPU, l'entra√Ænement sera plus lent")
        print("   Temps estim√© : 8-15 heures pour 50 epochs sur CPU")
    
    print("\n" + "="*70)
    
    return cuda_available


if __name__ == '__main__':
    gpu_available = check_gpu()
    sys.exit(0 if gpu_available else 1)
